# -*- coding: utf-8 -*-
"""Aurora_Dataset_final2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1auEBFsMds5U1NMDFnhHO1wR49uw1p5ag
"""

#1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re

#2
df=pd.read_csv("water_quality_train_.csv")
dftest = pd.read_csv("water_quality_test_.csv")

dftest

#3
#preview the dataset
df.head()

#4
#Seeing all the numeric columns

[col for col in df.columns if df[col].dtype!='O']

#5
df['Name of Monitoring Location'].value_counts()

#6
# Extract the source information from the full string into a new column

source_pattern = r'\b(HAND\s?PUMP|BORE\s?WELL|TUBE\s?WELL|GROUND\s?WATER|OPEN\s?WELL|INDUSTRIAL\s?AREA|\bWELL\b)\b'


df['Source'] = df['Name of Monitoring Location'].str.extract(source_pattern, flags=re.IGNORECASE)
dftest['Source'] = dftest['Name of Monitoring Location'].str.extract(source_pattern, flags=re.IGNORECASE)

"""### More about REGEX in python
https://www.w3schools.com/python/python_regex.asp
"""

#7
#Remove spaces from the string stored

df['Source'] = df['Source'].str.replace(r'\s+', '')
dftest['Source'] = dftest['Source'].str.replace(r'\s+', '')

#8
# Get the count of different sources

df['Source'].value_counts()

#9
df['Source'].isnull().sum()

#10
# Replace BDL values with 0 and ? with null

df.replace(['BDL','?'],[0,pd.NA],inplace=True)
dftest.replace(['BDL','?'],[0,pd.NA],inplace=True)

#11
# Get the count of different attributes

df['State Name'].value_counts()

#12
df['BOD_Min'].value_counts()

#13
# Drop the attribute Name of Monitoring Location as it is already replaced by source

df.drop(columns=['Name of Monitoring Location'],axis=1,inplace=True)
dftest.drop(columns=['Name of Monitoring Location'],axis=1,inplace=True)

#14
# Get the total count of null values in different attributes

df.isnull().sum()

#15
# Get the median value for all of the numeric columns

median=df[[val for val in df.columns if (val!='State Name' and val!='Source' and val!='Drinkability')]].median()
mediantest=dftest[[val for val in dftest.columns if (val!='State Name' and val!='Source')]].median()

#16
median

#17
#fill all the null values with the median values of the respective column

df.fillna(median,inplace=True)
dftest.fillna(mediantest,inplace=True)

#18
df['Temp_Max'].value_counts()

#19
df.isnull().sum()

#20
# Perform One hot encoding for the 'Source' column

encoded_data = pd.get_dummies(df['Source'], dummy_na=True, prefix='Type')
encoded_datatest = pd.get_dummies(dftest['Source'], dummy_na=True, prefix='Type')

# Concatenate the encoded data with the original DataFrame
df = pd.concat([df, encoded_data], axis=1)
dftest = pd.concat([dftest, encoded_datatest], axis=1)

# Drop the original 'Source' column if needed
df.drop('Source', axis=1,inplace=True)
dftest.drop('Source', axis=1,inplace=True)

#21
dftest.head()

dftest.head()

#22
df.describe()

#23
# Use LabelEncoder to encode the 'State' attribute to numeric values

from sklearn.preprocessing import LabelEncoder

#24
lb=LabelEncoder()
df['State Name N']=lb.fit_transform(df['State Name'])
dftest['State Name N']=lb.fit_transform(dftest['State Name'])

#25
df.drop(columns=['State Name'],axis=1,inplace=True)
dftest.drop(columns=['State Name'],axis=1,inplace=True)

#26
df.describe()

#27
# Convert the attributes to floating type

df[[val for val in df.columns if val!='Drinkability']]=df[[val for val in df.columns if val!='Drinkability']].astype(float)

#28
df.describe()
df.drop(columns='Station Code',axis=1,inplace=True)
stationcode = dftest['Station Code']
dftest.drop(columns='Station Code',axis=1,inplace=True)

#29
df1=df.copy()

#30
df.head()

#31
# Plot the different attributes to look for outliers

plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Temp_Min')
fig.set_title('')
fig.set_ylabel('Temp_Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Temp_Max')
fig.set_title('')
fig.set_ylabel('Temp_Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Conductivity_Max')
fig.set_title('')
fig.set_ylabel('Conductivity_Max')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Conductivity_Min')
fig.set_title('')
fig.set_ylabel('BOD_Min')
plt.show()

#32
# find outliers for different  variables

lower_limit=df.Temp_Max.mean()- 9*df.Temp_Max.std()
upper_limit=df.Temp_Max.mean()+ 9*df.Temp_Max.std()

#33
# drop the rows containing values outside the cap
df.loc[df['Temp_Max']>upper_limit,'Temp_Max']=upper_limit
df.loc[df['Temp_Max']<lower_limit,'Temp_Max']=lower_limit

#34
lower_limit=df.Conductivity_Min.mean()- 9*df.Conductivity_Min.std()
upper_limit=df.Conductivity_Min.mean()+ 9*df.Conductivity_Min.std()

#35
df.loc[df['Conductivity_Min']>upper_limit,'Conductivity_Min']=upper_limit
df.loc[df['Conductivity_Min']<lower_limit,'Conductivity_Min']=lower_limit

#36
lower_limit=df.Conductivity_Max.mean()- 9*df.Conductivity_Max.std()
upper_limit=df.Conductivity_Max.mean()+ 9*df.Conductivity_Max.std()

#37
df.loc[df['Conductivity_Max']>upper_limit,'Conductivity_Max']=upper_limit
df.loc[df['Conductivity_Max']<lower_limit,'Conductivity_Max']=lower_limit

#38
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Temp_Min')
fig.set_title('')
fig.set_ylabel('Temp_Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Temp_Max')
fig.set_title('')
fig.set_ylabel('Temp_Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Conductivity_Max')
fig.set_title('')
fig.set_ylabel('Conductivity_Max')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Conductivity_Min')
fig.set_title('')
fig.set_ylabel('Conductivity_Min')
plt.show()

#39
pd.set_option('display.max_columns', None)
df.describe()

#40
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='pH_Min')
fig.set_title('')
fig.set_ylabel('pH_Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='pH_Max')
fig.set_title('')
fig.set_ylabel('pH_Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='BOD_Min')
fig.set_title('')
fig.set_ylabel('BOD_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='DOB_Max')
fig.set_title('')
fig.set_ylabel('DOB_Max')
plt.show()

#41
q1=df.pH_Max.quantile(0.25)
q3=df.pH_Max.quantile(0.75)
IQR=q3-q1
lower_limit=q1- 10*IQR
upper_limit=q3 + 10*IQR

#42
df.drop(df[(df['pH_Min']>upper_limit)|(df['pH_Min']<lower_limit)].index,axis=0,inplace=True)

#43
df.loc[df['pH_Max']>upper_limit,'pH_Max']=upper_limit
df.loc[df['pH_Max']<lower_limit,'pH_Max']=lower_limit

#44
q1=df.pH_Min.quantile(0.25)
q3=df.pH_Min.quantile(0.75)
IQR=q3-q1
lower_limit=q1- 10*IQR
upper_limit=q3 + 10*IQR

#45
df.loc[df['pH_Min']>upper_limit,'pH_Min']=upper_limit
df.loc[df['pH_Min']<lower_limit,'pH_Min']=lower_limit

#46
q1=df['BOD_Min'].quantile(0.25)
q3=df['BOD_Min'].quantile(0.75)
IQR=q3-q1
lower_limit=q1- 3*IQR
upper_limit=q3 + 3*IQR

#47
df.loc[df['BOD_Min']>upper_limit,'BOD_Min']=upper_limit
df.loc[df['BOD_Min']<lower_limit,'BOD_Min']=lower_limit

#48
q1=df['DOB_Max'].quantile(0.25)
q3=df['DOB_Max'].quantile(0.75)
IQR=q3-q1
lower_limit=q1- 3*IQR
upper_limit=q3 + 3*IQR

#49
df.loc[df['DOB_Max']>upper_limit,'DOB_Max']=upper_limit
df.loc[df['DOB_Max']<lower_limit,'DOB_Max']=lower_limit

#50
plt.figure(figsize=(15,10))

plt.subplot(2, 2, 1)
fig = df.boxplot(column='pH_Min')
fig.set_title('')
fig.set_ylabel('pH_Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='pH_Max')
fig.set_title('')
fig.set_ylabel('pH_Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='BOD_Min')
fig.set_title('')
fig.set_ylabel('BOD_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='DOB_Max')
fig.set_title('')
fig.set_ylabel('DOB_Max')
plt.show()

#51
df.describe()

#52
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Nitrate?N + Nitrite_N Min')
fig.set_title('')
fig.set_ylabel('Nitrate?N + Nitrite_N Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Nitrate?N + Nitrite_N Max')
fig.set_title('')
fig.set_ylabel('Nitrate?N + Nitrite_N Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Fecal Coliform_Min')
fig.set_title('')
fig.set_ylabel('Fecal Coliform_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Fecal Coliform_Max')
fig.set_title('')
fig.set_ylabel('Fecal Coliform_Max')
plt.show()

#53
q1=df['Nitrate?N + Nitrite_N Max'].quantile(0.25)
q3=df['Nitrate?N + Nitrite_N Max'].quantile(0.75)
IQR=q3-q1
lower_limit=df['Nitrate?N + Nitrite_N Max'].mean()- 2*df['Nitrate?N + Nitrite_N Max'].std()
upper_limit=df['Nitrate?N + Nitrite_N Max'].mean()+ 3*df['Nitrate?N + Nitrite_N Max'].std()

#54
df.loc[df['Nitrate?N + Nitrite_N Max']>upper_limit,'Nitrate?N + Nitrite_N Max']=upper_limit
df.loc[df['Nitrate?N + Nitrite_N Max']<lower_limit,'Nitrate?N + Nitrite_N Max']=lower_limit

#55
lower_limit=df['Total Coliform_Max'].mean()- 2*df['Total Coliform_Max'].std()
upper_limit=df['Total Coliform_Max'].mean()+ 15*df['Total Coliform_Max'].std()

#56
df.loc[df['Total Coliform_Max']>upper_limit,'Total Coliform_Max']=upper_limit
df.loc[df['Total Coliform_Max']<lower_limit,'Total Coliform_Max']=lower_limit

#57
lower_limit=df['Fecal Coliform_Max'].mean()- 2*df['Fecal Coliform_Max'].std()
upper_limit=df['Fecal Coliform_Max'].mean()+ 30*df['Fecal Coliform_Max'].std()

#58
df.loc[df['Fecal Coliform_Max']>upper_limit,'Fecal Coliform_Max']=upper_limit
df.loc[df['Fecal Coliform_Max']<lower_limit,'Fecal Coliform_Max']=lower_limit

#59
q1=df['Fecal Coliform_Min'].quantile(0.25)
q3=df['Fecal Coliform_Min'].quantile(0.75)
IQR=q3-q1
lower_limit=df['Fecal Coliform_Min'].mean()- 2*df['Fecal Coliform_Min'].std()
upper_limit=df['Fecal Coliform_Min'].mean()+ 10*df['Fecal Coliform_Min'].std()

#60
df.loc[df['Fecal Coliform_Min']>upper_limit,'Fecal Coliform_Min']=upper_limit
df.loc[df['Fecal Coliform_Min']<lower_limit,'Fecal Coliform_Min']=lower_limit

#61
lower_limit=df['Total Dissolved Solids_Min'].mean()- 2*df['Total Dissolved Solids_Min'].std()
upper_limit=df['Total Dissolved Solids_Min'].mean()+ 5*df['Total Dissolved Solids_Min'].std()

#62
df.loc[df['Total Dissolved Solids_Min']>upper_limit,'Total Dissolved Solids_Min']=upper_limit
df.loc[df['Total Dissolved Solids_Min']<lower_limit,'Total Dissolved Solids_Min']=lower_limit

#63
lower_limit=df['Total Dissolved Solids_Max'].mean()- 2*df['Total Dissolved Solids_Max'].std()
upper_limit=df['Total Dissolved Solids_Max'].mean()+ 5*df['Total Dissolved Solids_Max'].std()

#64
df.loc[df['Total Dissolved Solids_Max']>upper_limit,'Total Dissolved Solids_Max']=upper_limit
df.loc[df['Total Dissolved Solids_Max']<lower_limit,'Total Dissolved Solids_Max']=lower_limit

#65
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Nitrate?N + Nitrite_N Min')
fig.set_title('')
fig.set_ylabel('Nitrate?N + Nitrite_N Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Nitrate?N + Nitrite_N Max')
fig.set_title('')
fig.set_ylabel('Nitrate?N + Nitrite_N Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Fecal Coliform_Min')
fig.set_title('')
fig.set_ylabel('Fecal Coliform_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Fecal Coliform_Max')
fig.set_title('')
fig.set_ylabel('Fecal Coliform_Max')
plt.show()

#66
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Nitrate?N + Nitrite_N Min')
fig.set_title('')
fig.set_ylabel('Nitrate?N + Nitrite_N Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Nitrate?N + Nitrite_N Max')
fig.set_title('')
fig.set_ylabel('Nitrate?N + Nitrite_N Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Total Coliform_Min')
fig.set_title('')
fig.set_ylabel('Total Coliform_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Total Coliform_Max')
fig.set_title('')
fig.set_ylabel('Total Coliform_Max')
plt.show()

#67
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Total Dissolved Solids_Min')
fig.set_title('')
fig.set_ylabel('Total Dissolved Solids_Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Total Dissolved Solids_Max')
fig.set_title('')
fig.set_ylabel('Total Dissolved Solids_Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Fluoride_Min')
fig.set_title('')
fig.set_ylabel('Fluoride_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Fluoride_Max')
fig.set_title('')
fig.set_ylabel('Fluoride_Max')
plt.show()

#68
q1=df['Fluoride_Min'].quantile(0.25)
q3=df['Fluoride_Min'].quantile(0.75)
IQR=q3-q1
lower_limit=q1- 2*IQR
upper_limit=q3 + 15*IQR

#69
df.loc[df['Fluoride_Min']>upper_limit,'Fluoride_Min']=upper_limit
df.loc[df['Fluoride_Min']<lower_limit,'Fluoride_Min']=lower_limit

#70
q1=df['Fluoride_Max'].quantile(0.25)
q3=df['Fluoride_Max'].quantile(0.75)
IQR=q3-q1
lower_limit=q1- 2*IQR
upper_limit=q3 + 15*IQR

#71
df.loc[df['Fluoride_Max']>upper_limit,'Fluoride_Max']=upper_limit
df.loc[df['Fluoride_Max']<lower_limit,'Fluoride_Max']=lower_limit

#72
plt.figure(figsize=(15,10))


plt.subplot(2, 2, 1)
fig = df.boxplot(column='Arsenic_Min')
fig.set_title('')
fig.set_ylabel('Arsenic_Min')


plt.subplot(2, 2, 2)
fig = df.boxplot(column='Arsenic_Max')
fig.set_title('')
fig.set_ylabel('Arsenic_Max')


plt.subplot(2, 2, 3)
fig = df.boxplot(column='Fluoride_Min')
fig.set_title('')
fig.set_ylabel('Fluoride_Min')


plt.subplot(2, 2, 4)
fig = df.boxplot(column='Fluoride_Max')
fig.set_title('')
fig.set_ylabel('Fluoride_Max')
plt.show()

#73
df.describe()

#74
import seaborn as sns

#75
df.shape

#76
lb=LabelEncoder()
df['DrinkabilityN']=lb.fit_transform(df['Drinkability'])

df1['DrinkabilityN']=lb.fit_transform(df1['Drinkability'])

df.drop(columns=['Drinkability'],axis=1,inplace=True)

df1.drop(columns=['Drinkability'],axis=1,inplace=True)

#77
df2=df['DrinkabilityN']
df3=df1['DrinkabilityN']
df.drop(columns=['DrinkabilityN'],axis=1,inplace=True)
df1.drop(columns=['DrinkabilityN'],axis=1,inplace=True)

#78
from sklearn.model_selection import train_test_split

#79
df2.value_counts()

#80
df.head(198)

#81
x_train,x_test,y_train,y_test=train_test_split(df,df2,test_size=0.3)

x_train1,x_test1,y_train1,y_test1=train_test_split(df1,df3,test_size=0.3)

"""# LOGISTIC REGRESSION"""

#82
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np

#83
from sklearn.linear_model import LogisticRegression


# instantiate the model
logit_model = LogisticRegression(solver='liblinear', random_state=0)

# fit the model
logit_model.fit(x_train, y_train)


# logit = sm.Logit(y_train, x_train)
# logit_model = logit.fit()

#86
y_pred_test = logit_model.predict(x_test)

y_pred_test

#87
# probability of getting output as 0 - not drinkable

logit_model.predict_proba(x_test)[:,0]

# how would the code for getting the probability for drinkable water look like?

#88
from sklearn.metrics import accuracy_score

print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))

#89
#comparing that with the training set

y_pred_train = logit_model.predict(x_train)

y_pred_train

#90
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))

#91
# checking for overfitting/underfitting

print('Training set score: {:.4f}'.format(logit_model.score(x_train, y_train)))

print('Test set score: {:.4f}'.format(logit_model.score(x_test, y_test)))





# Commented out IPython magic to ensure Python compatibility.
#92
import matplotlib.pyplot as plt
import seaborn as sn
# %matplotlib inline
from sklearn import metrics

#93
def draw_cm( actual, predicted ):
    ## Cret
    cm = metrics.confusion_matrix( actual, predicted)
    sn.heatmap(cm, annot=True,  fmt='.2f',
               xticklabels = ["POSITIVE", "NEGATIVE"] ,
               yticklabels = ["POSITIVE", "NEGATIVE"] )
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

#94
draw_cm( y_test,
        y_pred_test)

#95
print( metrics.classification_report( y_test,
                                        y_pred_test ) )



#96
def draw_roc( actual, probs ):
    fpr, \
    tpr, \
    thresholds = metrics.roc_curve( actual,
                                    probs,
                                    drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(8, 6))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.legend(loc="lower right")
    plt.show()

    return fpr, tpr, thresholds

#97
fpr, tpr, thresholds = draw_roc( y_test,
                                y_pred_test)

#98
auc_score = metrics.roc_auc_score( y_test,
                                    y_pred_test  )
round( float( auc_score ), 2 )

#99
tpr_fpr = pd.DataFrame( { 'tpr': tpr,
                         'fpr': fpr,
                         'thresholds': thresholds } )

tpr_fpr['diff'] = tpr_fpr.tpr - tpr_fpr.fpr
tpr_fpr.sort_values( 'diff', ascending = False )[0:5]

#100
from sklearn.model_selection import cross_val_predict

#101
predicted_proba_cv = cross_val_predict(logit_model, x_test, y_test, cv=5, method='predict_proba')

# Extract probabilities of the positive class
positive_proba_cv = predicted_proba_cv[:, 1]

#102
threshold = 0.6
predicted_custom_threshold = (positive_proba_cv >= threshold).astype(int)
predicted_custom_threshold

#103
auc_score = metrics.roc_auc_score( y_test,
                                    predicted_custom_threshold  )
round( float( auc_score ), 2 )

#104
cost_df = pd.DataFrame( columns = ['prob', 'cost'])



#105
draw_cm( y_test,
        predicted_custom_threshold )

"""# Decision Tree"""

#106
from sklearn.tree import DecisionTreeClassifier

clf_tree = DecisionTreeClassifier(criterion = 'gini',
                                  max_depth = 3 )

#107
clf_tree.fit( x_train, y_train )

#108
#!pip install graphviz

#109
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Assuming you have trained your decision tree classifier clf_tree

# Visualize the decision tree
plt.figure(figsize=(12, 8))
plot_tree(clf_tree, filled=True, feature_names=list(x_train.columns), class_names=['Drinkable', 'Not Drinkable'], rounded=True)
plt.show()

#110
tree_predict = clf_tree.predict( x_test )
metrics.roc_auc_score( y_test, tree_predict )

#111
print(y_train[y_train == 1].count(), y_train[y_train == 0].count())

"""## Why is this so good??"""

#112
gini_node_1 = 1 - pow(412/770, 2) - pow (358/770, 2)
print( round( gini_node_1, 4) )

#113
x_test.shape

#114
clf_tree_entropy = DecisionTreeClassifier( criterion = 'entropy',
                                          max_depth = 3 )
clf_tree_entropy.fit( x_train, y_train )

#115
import math
entropy_node_1 = - (491/700) * math.log2(491/700) - (209/700) * math.log2(209/700)
print( round( entropy_node_1, 2) )

#116
tree_predict = clf_tree_entropy.predict( x_test )
metrics.roc_auc_score( y_test, tree_predict )

#117
from sklearn.model_selection import GridSearchCV

tuned_parameters = [{'criterion': ['gini','entropy'],
                     'max_depth': range(2,10)}]


clf_tree = DecisionTreeClassifier()

clf = GridSearchCV(clf_tree,
                 tuned_parameters,
                 cv=10,
                 scoring='roc_auc')

clf.fit(x_train, y_train )

#118
clf.best_score_

#119
clf.best_params_







"""# K Nearest Neighbor

"""

#120
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import seaborn as sns



#121
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

# Assuming x_train is a Pandas DataFrame
# If it's a NumPy array, you can convert it to a DataFrame using pd.DataFrame(x_train)
# x_train = pd.DataFrame(x_train)

# Perform k-means clustering
data_for_clustering = x_train

# Create and fit the KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(data_for_clustering, y_train)
# Predict the labels for the data points
cluster_labels = knn.predict(x_train.values)

# Plot the clusters
plt.scatter(
    data_for_clustering.iloc[:, 0],
    data_for_clustering.iloc[:, 1],
    c=cluster_labels,
    edgecolor='black',
    cmap='viridis'
)
plt.title('KMeans Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

#122
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from mpl_toolkits.mplot3d import Axes3D

# Assuming x_train is a Pandas DataFrame
# If it's a NumPy array, you can convert it to a DataFrame using pd.DataFrame(x_train)
# x_train = pd.DataFrame(x_train)

# Perform k-means clustering
data_for_clustering = x_train

# Create and fit the KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(data_for_clustering, y_train)
# Predict the labels for the data points
cluster_labels = knn.predict(x_train.values)

# Plot the clusters in 3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(
    data_for_clustering.iloc[:, 0],
    data_for_clustering.iloc[:, 1],
    data_for_clustering.iloc[:, 2],  # Use the third feature for the z-axis
    c=cluster_labels,
    edgecolor='black',
    cmap='viridis'
)
ax.set_title('KMeans Clustering')
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_zlabel('Feature 3')

plt.show()

#123
import pandas as pd
from sklearn.cluster import KMeans
import plotly.graph_objs as go
from sklearn.neighbors import KNeighborsClassifier
from plotly.subplots import make_subplots

# Assuming x_train is a Pandas DataFrame
# If it's a NumPy array, you can convert it to a DataFrame using pd.DataFrame(x_train)
# x_train = pd.DataFrame(x_train)

# Perform k-means clustering
data_for_clustering = x_train

# Create and fit the KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(data_for_clustering, y_train)
# Predict the labels for the data points
cluster_labels = knn.predict(x_train.values)

# Create a Plotly figure with a 3D scatter plot
fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'scatter3d'}]])

fig.add_trace(
    go.Scatter3d(
        x=data_for_clustering.iloc[:, 0],
        y=data_for_clustering.iloc[:, 1],
        z=data_for_clustering.iloc[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=cluster_labels,  # Color by cluster labels
            colorscale='Viridis',
            opacity=0.8
        )
    )
)

# Update layout for better visibility
fig.update_layout(
    scene=dict(
        xaxis_title='Feature 1',
        yaxis_title='Feature 2',
        zaxis_title='Feature 3'
    ),
    title='KMeans Clustering'
)

fig.show()

#124
y_km = knn.predict(x_test.values)

#125
y_km

#126
y_test.values

#127
count= 0
for i,j in zip(y_km,y_test.values):
    if(i==j):
        count+=1
print(round(count*100/y_km.size,2))

"""# Neural Network"""

#128
import tensorflow as tf
from tensorflow.keras.layers import Dense,Input
from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy,BinaryCrossentropy

#129
#model.add(Input(X.shape))
model=Sequential()
model.add(tf.keras.layers.BatchNormalization())
model.add(Dense(512,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.02)))
model.add(tf.keras.layers.BatchNormalization())
model.add(Dense(256,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.02)))
model.add(tf.keras.layers.BatchNormalization())
model.add(Dense(256,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.02)))
model.add(tf.keras.layers.BatchNormalization())
#model.add(Dense(128,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)))
#model.add(tf.keras.layers.BatchNormalization())
model.add(Dense(64,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.02)))

model.add(Dense(1,activation='sigmoid'))

#130
model.compile(optimizer='adam',loss=BinaryCrossentropy(),metrics=['accuracy'])

#131
model.fit(x_train,y_train,epochs=200,verbose=True)

#132
model.evaluate(x_train,y_train)
model.evaluate(x_test1,y_test1)

x_test.columns

new_order = ['Temp_Min', 'Temp_Max', 'pH_Min', 'pH_Max', 'Conductivity_Min',
       'Conductivity_Max', 'BOD_Min', 'DOB_Max', 'Nitrate?N + Nitrite_N Min',
       'Nitrate?N + Nitrite_N Max', 'Fecal Coliform_Min', 'Fecal Coliform_Max',
       'Total Coliform_Min', 'Total Coliform_Max',
       'Total Dissolved Solids_Min', 'Total Dissolved Solids_Max',
       'Fluoride_Min', 'Fluoride_Max', 'Arsenic_Min', 'Arsenic_Max',
       'Type_BOREWELL', 'Type_GROUNDWATER', 'Type_HANDPUMP',
       'Type_INDUSTRIALAREA', 'Type_OPENWELL', 'Type_TUBEWELL', 'Type_WELL',
       'Type_nan', 'State Name N']
dftest['Type_BOREWELL'] = False
dftest['Type_OPEN WELL'] = False
dftest['Type_OPENWELL'] = False
dftest['Type_TUBEWELL'] = False
dftest = dftest[new_order]

predictions = clf.predict(dftest)
def change(x):
    if x:
        return 'Not Drinkable'
    else:
        return 'Drinkable'
# Convert predictions into a DataFrame
predictions_df = pd.DataFrame(predictions, columns=['Predicted_Output'])
predictions_df['Station Code'] = stationcode
predictions_df = predictions_df[['Station Code',"Predicted_Output"]]
predictions_df.set_index(['Station Code'],inplace = True)
predictions_df['Predicted_Output'] = predictions_df['Predicted_Output'].apply(change)
predictions_df.rename(columns={'Predicted_Output':"Drinkability"},inplace=True)
predictions_df.to_csv('samplee.csv')



dftest.to_csv('sample.csv')